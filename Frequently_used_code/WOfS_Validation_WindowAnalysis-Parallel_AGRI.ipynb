{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WOfS Validation Data Analysis - Window-based and Parallel  <img align=\"right\" src=\"../Supplementary_data/DE_Africa_Logo_Stacked_RGB_small.jpg\">\n",
    "\n",
    "* **Products used:** \n",
    "[ga_ls8c_wofs_2](https://explorer.digitalearth.africa/ga_ls8c_wofs_2),\n",
    "[ga_ls8c_wofs_2_summary ](https://explorer.digitalearth.africa/ga_ls8c_wofs_2_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "The [Water Observations from Space (WOfS)](https://www.ga.gov.au/scientific-topics/community-safety/flood/wofs/about-wofs) is a derived product from Landsat 8 satellite observations as part of provisional Landsat 8 Collection 2 surface reflectance and shows surface water detected in Africa.\n",
    "Individual water classified images are called Water Observation Feature Layers (WOFLs), and are created in a 1-to-1 relationship with the input satellite data. \n",
    "Hence there is one WOFL for each satellite dataset processed for the occurrence of water.\n",
    "\n",
    "The data in a WOFL is stored as a bit field. This is a binary number, where each digit of the number is independantly set or not based on the presence (1) or absence (0) of a particular attribute (water, cloud, cloud shadow etc). In this way, the single decimal value associated to each pixel can provide information on a variety of features of that pixel. \n",
    "For more information on the structure of WOFLs and how to interact with them, see [Water Observations from Space](../Datasets/Water_Observations_from_Space.ipynb) and [Applying WOfS bitmasking](../Frequently_used_code/Applying_WOfS_bitmasking.ipynb) notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook explains how you can perform validation analysis for WOFS derived product using collected ground truth dataset and window-based sampling. \n",
    "\n",
    "The notebook demonstrates how to:\n",
    "\n",
    "1. Load validation points for each partner institutions following cleaning stage as an ESRI shapefile\n",
    "2. Query WOFL data for validation points and capture available WOfS observation available\n",
    "3. Extract statistics for each WOfS observation in each validation point using a 3 by 3 window and multiprocessing functionality \n",
    "4. Extract a LUT for each point that contains both validation info and WOfS result for each month \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell.\n",
    "\n",
    "After finishing the analysis, you can modify some values in the \"Analysis parameters\" cell and re-run the analysis to load WOFLs for a different location or time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Import Python packages that are used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time \n",
    "import datacube\n",
    "from datacube.utils import masking, geometry \n",
    "import sys\n",
    "import os\n",
    "import dask \n",
    "import rasterio, rasterio.features\n",
    "import xarray\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import geopandas as gpd\n",
    "import subprocess as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy, scipy.ndimage\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #this will suppress the warnings for multiple UTM zones in your AOI \n",
    "\n",
    "sys.path.append(\"../Scripts\")\n",
    "from rasterio.mask import mask\n",
    "from geopandas import GeoSeries, GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "from deafrica_plotting import map_shapefile,display_map, rgb\n",
    "from deafrica_spatialtools import xr_rasterize\n",
    "from deafrica_datahandling import wofs_fuser, mostcommon_crs,load_ard,deepcopy\n",
    "from deafrica_dask import create_local_dask_cluster\n",
    "\n",
    "#for parallelisation \n",
    "from multiprocessing import Pool, Manager\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube\n",
    "Activate the datacube database, which provides functionality for loading and displaying stored Earth observation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='WOfS_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyse validation points collected by each partner institution, we need to obtain WOfS surface water observation data that corresponds with the labelled input data locations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load validation points for each partner institutions as a list of observations each has a location and month\n",
    "    * Load the cleaned validation file as ESRI `shapefile`\n",
    "    * Inspect the shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the joint csv file to shapefile \n",
    "# df = pd.read_csv('../Supplementary_data/Validation/Refined/CEO_AFRIGIST_2020-09-15.csv')\n",
    "\n",
    "# geometry = [Point(xy) for xy in zip(df.LON, df.LAT)]\n",
    "# crs = {'init': 'epsg:4326'} #http://www.spatialreference.org/ref/epsg/2263/\n",
    "# geo_df = GeoDataFrame(df, crs=crs, geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geo_df.to_file(filename='../Supplementary_data/Validation/Refined/groundtruth_AFRIGIST.shp') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '../Supplementary_data/Validation/Refined/shapefiles/AFRI_subset.shp'\n",
    "path = '../Supplementary_data/Validation/Refined/groundtruth_AFRIGIST.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed_ 0', 'Unnamed__1', 'PLOT_ID', 'LON', 'LAT', 'FLAGGED',\n",
       "       'ANALYSES', 'SENTINEL2Y', 'STARTDATE', 'ENDDATE', 'WATER', 'NO_WATER',\n",
       "       'BAD_IMAGE', 'NOT_SURE', 'CLASS', 'COMMENT', 'MONTH', 'WATERFLAG',\n",
       "       'geometry'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = gpd.read_file(path).to_crs('epsg:6933') #reading the table and converting CRS to metric \n",
    "input_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data= input_data.drop(['Unnamed_ 0','Unnamed__1','STARTDATE','ENDDATE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11334, 15)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = [(x,y) for x, y in zip(input_data.geometry.x, input_data.geometry.y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample WOfS at the ground truth coordinates in 3 by 3 window size\n",
    "To load WOFL data, we can first create a re-usable query as below that will define the time period we are interested in, as well as other important parameters that are used to correctly load the data. \n",
    "\n",
    "As WOFLs are created scene-by-scene, and some scenes overlap, it's important when loading data to `group_by` solar day, and ensure that the data between scenes is combined correctly by using the WOfS `fuse_func`.\n",
    "This will merge observations taken on the same day, and ensure that important data isn't lost when overlapping datasets are combined.\n",
    "\n",
    "We can convert the WOFL bit field into a binary array containing True and False values. This allows us to use the WOFL data as a mask that can be applied to other datasets.\n",
    "The `make_mask` function allows us to create a mask using the flag labels (e.g. \"wet\" or \"dry\") rather than the binary numbers we used above. For more details on how to do masking on WOfS, see the [Applying_WOfS_bit_masking](../Frequently_used_code/Applying_WOfS_bitmasking.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate query object \n",
    "query ={'group_by':'solar_day',\n",
    "        'resampling':'nearest'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wofs_for_point(index, row, input_data, query, results_wet, results_clear):\n",
    "    dc = datacube.Datacube(app='WOfS_accuracy')\n",
    "    #get the month value for each index\n",
    "    month = input_data.loc[index]['MONTH'] \n",
    "    plot_id = input_data.loc[index]['PLOT_ID']\n",
    "    #set the time for query of the WOfS database according to the month value in the validation table \n",
    "    time = '2018-' + f'{month:02d}' \n",
    "    #having the original query as it is \n",
    "    dc_query = deepcopy(query) \n",
    "    geom = geometry.Geometry(input_data.geometry.values[index].__geo_interface__,  geometry.CRS('EPSG:6933'))\n",
    "    q = {\"geopolygon\":geom}\n",
    "    t = {\"time\":time} \n",
    "    dc_query.update(t)\n",
    "    dc_query.update(q)\n",
    "    wofls = dc.load(product =\"ga_ls8c_wofs_2\",\n",
    "                    y = (input_data.geometry.y[index] - 30.5, input_data.geometry.y[index] + 30.5),\n",
    "                    x =(input_data.geometry.x[index] - 30.5, input_data.geometry.x[index] + 30.5),\n",
    "                    crs = 'EPSG:6933',\n",
    "                    time=time,\n",
    "                    output_crs = 'EPSG:6933',\n",
    "                    resolution=(-30,30))\n",
    "\n",
    "    #Define a mask for wet and clear pixels \n",
    "    wet_nocloud = {\"water_observed\":True, \"cloud_shadow\":False, \"cloud\":False,\"nodata\":False}\n",
    "    #Define a mask for dry and clear pixels \n",
    "    dry_nocloud = {\"water_observed\":False, \"cloud_shadow\":False, \"cloud\":False, \"nodata\":False}\n",
    "    wofl_wetnocloud = masking.make_mask(wofls, **wet_nocloud).astype(int) \n",
    "    wofl_drynocloud = masking.make_mask(wofls, **dry_nocloud).astype(int)\n",
    "    clear = (wofl_wetnocloud | wofl_drynocloud).water.all(dim=['x','y']).values\n",
    "    n_clear = clear.sum() #record this and use it to filter out month with no valid data \n",
    "    if n_clear > 0:\n",
    "        wet = wofl_wetnocloud.isel(time=clear).water.max().values  #record this as WOfS has seen water in the 3*3 window\n",
    "    else:\n",
    "        wet = 0 \n",
    "\n",
    "    results_wet.update({str(int(plot_id))+\"_\"+str(month) : int(wet)})\n",
    "    results_clear.update({str(int(plot_id))+\"_\"+str(month) : int(n_clear)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parallel_fun(input_data, query, ncpus):\n",
    "    \n",
    "    manager = mp.Manager()\n",
    "    results_wet = manager.dict()\n",
    "    results_clear = manager.dict()\n",
    "   \n",
    "    # progress bar\n",
    "    pbar = tqdm(total=len(input_data))\n",
    "        \n",
    "    def update(*a):\n",
    "        pbar.update()\n",
    "\n",
    "    with mp.Pool(ncpus) as pool:\n",
    "        for index, row in input_data.iterrows():\n",
    "            pool.apply_async(get_wofs_for_point,\n",
    "                                 [index,\n",
    "                                 row,\n",
    "                                 input_data,\n",
    "                                 query,\n",
    "                                 results_wet,\n",
    "                                 results_clear], callback=update)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        pbar.close()\n",
    "        \n",
    "    return results_wet, results_clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The following cell can take several minutes to run.** If you set `ncpus > 1`, then this function will be run in parallel across the specified number of processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 11267/11334 [33:03<00:11,  5.68it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32min 3s, sys: 46.7 s, total: 32min 50s\n",
      "Wall time: 33min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wet, clear= _parallel_fun(input_data, query, ncpus=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert results of our dataset to neat pandas df\n",
    "wetdf = pd.DataFrame.from_dict(wet, orient = 'index')\n",
    "cleardf = pd.DataFrame.from_dict(clear,orient='index')\n",
    "df2 = wetdf.merge(cleardf, left_index=True, right_index=True)\n",
    "#print(df2.shape)\n",
    "df2 = df2.rename(columns={'0_x':'CLASS_WET','0_y':'CLEAR_OBS'})\n",
    "#split the index (which is plotid+month) into seperate columns\n",
    "for index, row in df2.iterrows():\n",
    "    df2.at[index,'PLOT_ID'] = index.split('_')[0] +'.0'\n",
    "    df2.at[index,'MONTH'] = index.split('_')[1]\n",
    "#rest the index\n",
    "df2 = df2.reset_index(drop=True)\n",
    "#convert plot id and month to str to help with matching\n",
    "input_data['PLOT_ID'] = input_data.PLOT_ID.astype(str)\n",
    "input_data['MONTH']= input_data.MONTH.astype(str)\n",
    "# merge both dataframe at locations where plotid and month match\n",
    "final_df = pd.merge(input_data, df2, on=['PLOT_ID','MONTH'], how='outer')\n",
    "#final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As water flag more than 1 and also clear observation equal to zero \n",
    "indexNames = final_df[(final_df['WATERFLAG'] > 1) | (final_df['CLEAR_OBS'] == 0)].index \n",
    "final_df.drop(indexNames, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5061, 17)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will count on the number of month for each plotID in the final table \n",
    "count = final_df.groupby(['PLOT_ID'])['MONTH'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the check on the count for each plot id as a csv file \n",
    "count.to_csv('../Supplementary_data/Validation/Refined/final_AFRIGIST_count_w305P.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(('../Supplementary_data/Validation/Refined/ground_truth_AFRIGIST_W305mfinal_parallel.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** January 2020\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DE Africa User Guide's [Tags Index](https://) (placeholder as this does not exist yet)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**:  :index:`WOfS`, :index:`fractional cover`, :index:`deafrica_plotting`, :index:`deafrica_datahandling`, :index:`display_map`, :index:`wofs_fuser`, :index:`WOFL`, :index:`masking`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review for further development \n",
    "* the following cells were tested for the RCMRD dataset following query WOfS for both wet and clear but it didnt improve the timing compared to the previous code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wetDF = pd.DataFrame.from_dict(wet, orient = 'index')\n",
    "#clearDF = pd.DataFrame.from_dict(clear,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mergedDF = wetDF.merge(clearDF, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mergedDF = mergedDF.rename(columns={'0_x':'CLASS_WET','0_y':'CLEAR_OBS'})\n",
    "#mergedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in mergedDF.iterrows():\n",
    "#     mergedDF.at[index,'ID'] = index.split('_')[0] +'.0'\n",
    "#     mergedDF.at[index,'M'] = index.split('_')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mergedDF.reset_index(drop=True, inplace=True)\n",
    "#mergedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in input_data.iterrows():\n",
    "#     for i, j in mergedDF.iterrows():\n",
    "#         if (input_data.at[index,'PLOT_ID'] == mergedDF.loc[i]['ID'] and input_data.loc[index]['MONTH'] == mergedDF.loc[i]['M']):\n",
    "#             input_data.at[index,'CLASS_WET'] = mergedDF.loc[i]['CLASS_WET']\n",
    "#             input_data.at[index,'CLEAR_OBS'] = mergedDF.loc[i]['CLEAR_OBS']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "9e3fa49adf8c4170abfcd954c2ec045a": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletZoomControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "zoom_in_text",
        "zoom_in_title",
        "zoom_out_text",
        "zoom_out_title"
       ]
      }
     },
     "dc642f11c1fb492ca419b0ed6fc4f8c3": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletAttributionControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "prefix"
       ],
       "position": "bottomright",
       "prefix": "Leaflet"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
